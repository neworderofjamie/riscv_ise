{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP03CpuJfUlIvrsrQI5CW1u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neworderofjamie/riscv_ise/blob/compiler/mnist_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are lots of rough edges here: error checking is lacking in places, the compiler supports an even smaller subset of C than it should and the wrapping of various bits of API is not very Pythonic.\n",
        "\n",
        "# Installation\n",
        "The current prototype FeNN toolchain is a little bit tricky to build as it re-uses parts of GeNN (mostly the type system and the GeNNCode scanner, parser and type checker) so, on colab, we can install a prebuilt wheel from my google drive:"
      ],
      "metadata": {
        "id": "zpi4-Zy__TDB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE_bp-iU2ZaT",
        "outputId": "6de654b8-73dd-40bc-896e-ee4d82e8dead"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hEx5nI2ITfmrrjfidr5y1SyWnsjFI8Qq\n",
            "To: /content/pyfenn-0.0.1-cp311-cp311-linux_x86_64.whl\n",
            "\r  0% 0.00/6.37M [00:00<?, ?B/s]\r100% 6.37M/6.37M [00:00<00:00, 199MB/s]\n",
            "Processing ./pyfenn-0.0.1-cp311-cp311-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from pyfenn==0.0.1) (2.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from pyfenn==0.0.1) (5.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pyfenn==0.0.1) (75.2.0)\n",
            "pyfenn is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
          ]
        }
      ],
      "source": [
        "if \"google.colab\" in str(get_ipython()):\n",
        "    !gdown 1hEx5nI2ITfmrrjfidr5y1SyWnsjFI8Qq\n",
        "    !pip install pyfenn-0.0.1-cp311-cp311-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/neworderofjamie/riscv_ise/raw/refs/heads/compiler/bin/mnist_bias.bin\n",
        "!wget https://github.com/neworderofjamie/riscv_ise/raw/refs/heads/compiler/bin/mnist_in_hid.bin\n",
        "!wget https://github.com/neworderofjamie/riscv_ise/raw/refs/heads/compiler/bin/mnist_hid_out.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2Nco5kXEG1w",
        "outputId": "e7828dc3-641a-486a-8533-70fc0c68ea9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-28 15:45:48--  https://github.com/neworderofjamie/riscv_ise/raw/refs/heads/compiler/bin/mnist_bias.bin\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/neworderofjamie/riscv_ise/refs/heads/compiler/bin/mnist_bias.bin [following]\n",
            "--2025-04-28 15:45:48--  https://raw.githubusercontent.com/neworderofjamie/riscv_ise/refs/heads/compiler/bin/mnist_bias.bin\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 64 [application/octet-stream]\n",
            "Saving to: â€˜mnist_bias.bin.1â€™\n",
            "\n",
            "\rmnist_bias.bin.1      0%[                    ]       0  --.-KB/s               \rmnist_bias.bin.1    100%[===================>]      64  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-28 15:45:48 (3.30 MB/s) - â€˜mnist_bias.bin.1â€™ saved [64/64]\n",
            "\n",
            "--2025-04-28 15:45:48--  https://github.com/neworderofjamie/riscv_ise/raw/refs/heads/compiler/bin/mnist_in_hid.bin\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/neworderofjamie/riscv_ise/refs/heads/compiler/bin/mnist_in_hid.bin [following]\n",
            "--2025-04-28 15:45:48--  https://raw.githubusercontent.com/neworderofjamie/riscv_ise/refs/heads/compiler/bin/mnist_in_hid.bin\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 200704 (196K) [application/octet-stream]\n",
            "Saving to: â€˜mnist_in_hid.bin.1â€™\n",
            "\n",
            "mnist_in_hid.bin.1  100%[===================>] 196.00K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-04-28 15:45:48 (5.53 MB/s) - â€˜mnist_in_hid.bin.1â€™ saved [200704/200704]\n",
            "\n",
            "--2025-04-28 15:45:48--  https://github.com/neworderofjamie/riscv_ise/raw/refs/heads/compiler/bin/mnist_hid_out.bin\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/neworderofjamie/riscv_ise/refs/heads/compiler/bin/mnist_hid_out.bin [following]\n",
            "--2025-04-28 15:45:49--  https://raw.githubusercontent.com/neworderofjamie/riscv_ise/refs/heads/compiler/bin/mnist_hid_out.bin\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8192 (8.0K) [application/octet-stream]\n",
            "Saving to: â€˜mnist_hid_out.bin.1â€™\n",
            "\n",
            "mnist_hid_out.bin.1 100%[===================>]   8.00K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-28 15:45:49 (17.7 MB/s) - â€˜mnist_hid_out.bin.1â€™ saved [8192/8192]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the trusty mnist package so we can easily access a dataset:"
      ],
      "metadata": {
        "id": "qCuUjyrm_pZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mnist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYEvCEXj7LWT",
        "outputId": "6fa4c7ba-9e2e-49fd-ce23-63346bc10552"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: mnist in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from mnist) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n",
        "Import a bunch of stuff from PyFeNN:"
      ],
      "metadata": {
        "id": "4cgxlc-R_vCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mnist\n",
        "import numpy\n",
        "import numpy as np\n",
        "\n",
        "from pyfenn import (BackendFeNNSim, EventContainer, Model, ProcessGroup,\n",
        "                    Runtime, Shape)\n",
        "from pyfenn import (EventContainer, EventPropagationProcess,\n",
        "                    NeuronUpdateProcess, NumericValue, Parameter,\n",
        "                    RNGInitProcess, Shape, UnresolvedType, Variable)\n",
        "\n",
        "from pyfenn import init_logging\n",
        "from pyfenn.utils import get_array_view, get_latency_spikes, load_and_push, zero_and_push"
      ],
      "metadata": {
        "id": "TUMEw7-x4buT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer classes\n",
        "FeNN is programmed using a small number of primitive objects:\n",
        "*   ``Processes`` perform computation\n",
        "*   ``Variables`` are used to hold model state e.g. neuron variables and weights\n",
        "*   ``EventContainers`` are the primary means of communication between neuron processes\n",
        "\n",
        "The FeNN tools don't really enforce any particular style of modelling but you can easily use these primitives to create PyTorchesque layer objects. We start by creating a leaky integrator for the output layer. This integrates an input current + bias into a membrane voltage which is averaged over the trial. The update to be performed each timestep is implemented in a ``NeuronUpdateProcess`` which performs the same update to each neuron (as dictated by the same of the variables). In future, these processes might be Just-in-Time compiled from Python but, right now, they are implemented in [GeNNCode](https://genn-team.github.io/genn/documentation/5/custom_models.html#genncode). This is basically a subset of C with extensions for fixed-point types inspired by the [ISO standard extension](https://www.open-std.org/jtc1/sc22/wg14/www/docs/n1005.pdf). In the ``LI`` model, this is most obvious is the ``0.0h6`` literal suffix which indicates that this is a fixed point literal with 6 fractional bits (type promotion doesn't work 100% right now...):"
      ],
      "metadata": {
        "id": "DR1TCosBFf-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LI:\n",
        "    def __init__(self, shape, tau_m: float, num_timesteps: int):\n",
        "        self.shape = Shape(shape)\n",
        "        dtype = UnresolvedType(\"s9_6_sat_t\")\n",
        "\n",
        "        self.v = Variable(self.shape, dtype)\n",
        "        self.i = Variable(self.shape, dtype)\n",
        "        self.v_avg = Variable(self.shape, dtype)\n",
        "        self.bias = Variable(self.shape, dtype)\n",
        "        self.process = NeuronUpdateProcess(\n",
        "            \"\"\"\n",
        "            V = (Alpha * V) + I + Bias;\n",
        "            I = 0.0h6;\n",
        "            VAvg += (VAvgScale * V);\n",
        "            \"\"\",\n",
        "            {\"Alpha\": Parameter(NumericValue(np.exp(-1.0 / tau_m)), dtype),\n",
        "             \"VAvgScale\": Parameter(NumericValue(1.0 / (num_timesteps / 2)), dtype)},\n",
        "            {\"V\": self.v, \"VAvg\": self.v_avg, \"I\": self.i, \"Bias\": self.bias})\n"
      ],
      "metadata": {
        "id": "wGOEN7Ow6nCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Leaky Integrate-and-Fire model we use for the hidden layer is slightly more complex, but is defined in basically the same way. Because the LIF neuron emits spikes, as well as variables, it has an ``EventContainer`` to manage the emitted spike. In the process code, events are emitted by calling the name of assigned to the event container i.e. ``Spike()``:"
      ],
      "metadata": {
        "id": "u_brKjkxIqdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LIF:\n",
        "    def __init__(self, shape, tau_m: float, tau_refrac: int, v_thresh: float):\n",
        "        self.shape = Shape(shape)\n",
        "        dtype = UnresolvedType( \"s10_5_sat_t\")\n",
        "        self.v = Variable(self.shape, dtype)\n",
        "        self.i = Variable(self.shape, dtype)\n",
        "        self.refrac_time = Variable(self.shape, UnresolvedType(\"int16_t\"))\n",
        "        self.out_spikes = EventContainer(self.shape)\n",
        "        self.process = NeuronUpdateProcess(\n",
        "            \"\"\"\n",
        "            V = (Alpha * V) + I;\n",
        "            I = 0.0h5;\n",
        "            if (RefracTime > 0) {\n",
        "               RefracTime -= 1;\n",
        "            }\n",
        "            else if(V >= VThresh) {\n",
        "               Spike();\n",
        "               V -= VThresh;\n",
        "               RefracTime = TauRefrac;\n",
        "            }\n",
        "            \"\"\",\n",
        "            {\"Alpha\": Parameter(NumericValue(np.exp(-1.0 / tau_m)), dtype),\n",
        "             \"VThresh\": Parameter(NumericValue(v_thresh), dtype),\n",
        "             \"TauRefrac\": Parameter(NumericValue(tau_refrac), UnresolvedType(\"int16_t\"))},\n",
        "            {\"V\": self.v, \"I\": self.i, \"RefracTime\": self.refrac_time},\n",
        "            {\"Spike\": self.out_spikes})"
      ],
      "metadata": {
        "id": "_KsNi8Ov6ddG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Synapse updates are also fully programmable but this is currently not exposed. All that is currently exposed is a event-driven spike propagation process which takes an ``EventContainer`` of events and propagates them through a ``Variable`` of weights and writes the accumulated result to a target variable:"
      ],
      "metadata": {
        "id": "eC40qOmdHH8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear:\n",
        "    def __init__(self, source_events: EventContainer, target_var: Variable,\n",
        "                 weight_dtype: str):\n",
        "        self.shape = Shape([source_events.shape.num_neurons,\n",
        "                            target_var.shape.num_neurons])\n",
        "        weight_dtype = UnresolvedType(weight_dtype)\n",
        "\n",
        "        self.weight = Variable(self.shape, weight_dtype)\n",
        "        self.process = EventPropagationProcess(source_events, self.weight,\n",
        "                                               target_var)\n"
      ],
      "metadata": {
        "id": "dqE5B5B-8_Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ],
      "metadata": {
        "id": "ndJkztL9FkQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples = 10000\n",
        "num_timesteps = 79\n",
        "input_shape = [28 * 28]\n",
        "hidden_shape = [128]\n",
        "output_shape = [10]\n",
        "input_hidden_shape = [28 * 28, 128]\n",
        "hidden_output_shape = [128, 10]"
      ],
      "metadata": {
        "id": "AR1n7maS6tb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "Convert MNIST into a latency. Yan LeCun's original site has been down for some time/blocking colab so we override"
      ],
      "metadata": {
        "id": "E49PKLzA64O4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist.datasets_url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
        "mnist_spikes = get_latency_spikes(mnist.test_images())\n",
        "mnist_labels = mnist.test_labels().astype(np.int16)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1qrVC-x6taL",
        "outputId": "44b61173-9a47-4127-de8f-b101496da75e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pyfenn/utils.py:59: RuntimeWarning: divide by zero encountered in log\n",
            "  times = np.round(tau * np.log(i / (i - threshold))).astype(int)\n",
            "/usr/local/lib/python3.11/dist-packages/pyfenn/utils.py:59: RuntimeWarning: invalid value encountered in cast\n",
            "  times = np.round(tau * np.log(i / (i - threshold))).astype(int)\n",
            "/usr/local/lib/python3.11/dist-packages/pyfenn/utils.py:59: RuntimeWarning: divide by zero encountered in divide\n",
            "  times = np.round(tau * np.log(i / (i - threshold))).astype(int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model definition\n",
        "The FeNN tools can produce lots of helpful logging information so we initialise this system before we do anything else (if you use ``from pyfenn import PlogSeverity`` to import the enum you can then use e.g. ``PlogSeverity.DEBUG`` to control the logging level):"
      ],
      "metadata": {
        "id": "6Q4AILkYCEmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_logging()"
      ],
      "metadata": {
        "id": "-z5TKUCLCByo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input spikes can be directly injected into FeNN rather than needing any sort of layer so define an EventContainer to hold them"
      ],
      "metadata": {
        "id": "b4tz2VYN7Wlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_spikes = EventContainer(Shape(input_shape), num_timesteps)"
      ],
      "metadata": {
        "id": "wEE6sPpe7V5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then create hidden and output layers using the classes we defined above. The fixed-point types are specified as strings, for example s10_5_sat_t is a signed 16-bit fixed point type (this is all FeNN currently supports) with 10 integer and 5 fractional bits to which saturation should be applied (currently only when adding and subtracting):"
      ],
      "metadata": {
        "id": "WgjyF0qj7mOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden = LIF(hidden_shape, 20.0, 5, 0.61)\n",
        "output = LI(output_shape, 20.0, num_timesteps)"
      ],
      "metadata": {
        "id": "NLIaZc0O6x6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we connect spiking outputs to input variables using the linear layer class we defined earlier:"
      ],
      "metadata": {
        "id": "IQXNwWGB8zi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_hidden = Linear(input_spikes, hidden.i, \"s10_5_sat_t\")\n",
        "hidden_output = Linear(hidden.out_spikes, output.i, \"s9_6_sat_t\")"
      ],
      "metadata": {
        "id": "oQBHvf-W8ets"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process groups define computation that can be performed in parallel (in fact, on FeNN it's not but this won't be the case with e.g. GPU backends) so we group our neuron update processes and event propagation processes into seperate groups"
      ],
      "metadata": {
        "id": "GsqvoBeZ9O86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neuron_update_processes = ProcessGroup([hidden.process, output.process])\n",
        "synapse_update_processes = ProcessGroup([input_hidden.process, hidden_output.process])"
      ],
      "metadata": {
        "id": "5GJbeSgS9Hr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define a model which groups together all parts of our simulation:"
      ],
      "metadata": {
        "id": "hXDuBp2B9kKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model([neuron_update_processes, synapse_update_processes])"
      ],
      "metadata": {
        "id": "-d42nihX9gbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulation\n",
        "Sadly Google has yet to install FeNN nodes into it's cloud so for now we create a simulation backend (if you are lucky enough to be running on a Kria KV260 with the bitstream loaded, ``BackendFeNNHW`` would be what you need) and use it to create a generic simulation kernel. The control flow of these kernels *will* be fully programmable but for now you can either create a really simple kernel which just runs a list of process groups or a 'simulation' kernel which offloads running a loop over time with a list of process groups in the body and another list that runs at the end (for example to copy data off of FeNN)"
      ],
      "metadata": {
        "id": "trpGDMqC9zwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend = BackendFeNNSim()\n",
        "code = backend.generate_simulation_kernel([synapse_update_processes, neuron_update_processes],\n",
        "                                          [],\n",
        "                                          num_timesteps, model)"
      ],
      "metadata": {
        "id": "5qTO9zPZ9v1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have some code, we create a ``Runtime`` object to interact with the FeNN. We first use this to allocate the memory required for our model on FeNN:"
      ],
      "metadata": {
        "id": "y94UlN66_7XS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "runtime = Runtime(model, backend)\n",
        "runtime.allocate()"
      ],
      "metadata": {
        "id": "oAU6n6xN_Kfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we use some helper functions to load weights into the appropriate variables:"
      ],
      "metadata": {
        "id": "IenSfYE5AneX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_and_push(\"mnist_in_hid.bin\", input_hidden.weight, runtime)\n",
        "load_and_push(\"mnist_hid_out.bin\", hidden_output.weight, runtime)\n",
        "load_and_push(\"mnist_bias.bin\", output.bias, runtime)"
      ],
      "metadata": {
        "id": "bdYJRLN8Agsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and set the remaining variables to zero:"
      ],
      "metadata": {
        "id": "YcJqRtw2ArbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zero_and_push(hidden.v, runtime)\n",
        "zero_and_push(hidden.i, runtime)\n",
        "zero_and_push(hidden.refrac_time, runtime)\n",
        "zero_and_push(output.v, runtime)\n",
        "zero_and_push(output.i, runtime)\n",
        "zero_and_push(output.v_avg, runtime)"
      ],
      "metadata": {
        "id": "ZYhU6-15AwdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we upload the code generated by the backend to FeNN:"
      ],
      "metadata": {
        "id": "Cp8gi-giA0Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "runtime.set_instructions(code)"
      ],
      "metadata": {
        "id": "t6qjbAi9AzyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ``Runtime`` object creates a bunch of 'Array' objects which are used to interact with model state at runtime. To save typing later on, we look these up now:"
      ],
      "metadata": {
        "id": "_DFSZgyEA-kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_spike_array, input_spike_view = get_array_view(runtime, input_spikes,\n",
        "                                                     np.uint32)\n",
        "hidden_spike_array = runtime.get_array(hidden.out_spikes)\n",
        "\n",
        "output_v_avg_array, output_v_avg_view = get_array_view(runtime, output.v_avg,\n",
        "                                                       np.int16)"
      ],
      "metadata": {
        "id": "yiyDQpO2A9KC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we're ready to go! Now we can loop through the MNIST digits and:\n",
        "1.   Copy each digit into the input spike array\n",
        "2.   Run the kernel\n",
        "3.   Copy the averaged output voltage back from FeNN\n",
        "4.   Check whether this matches the correct label"
      ],
      "metadata": {
        "id": "-81zZEvlBWa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_correct = 0\n",
        "for i in range(num_examples):\n",
        "    # Copy data to array host pointe\n",
        "    input_spike_view[:] = mnist_spikes[i]\n",
        "    input_spike_array.push_to_device();\n",
        "\n",
        "    # Classify\n",
        "    runtime.run()\n",
        "\n",
        "    # Copy output V sum from device\n",
        "    output_v_avg_array.pull_from_device();\n",
        "\n",
        "    # Determine if output is correct\n",
        "    classification = np.argmax(output_v_avg_view)\n",
        "    if classification == mnist_labels[i]:\n",
        "        num_correct += 1\n",
        "\n",
        "    # Zero output and push\n",
        "    output_v_avg_view[:] = 0\n",
        "    output_v_avg_array.push_to_device()\n",
        "\n",
        "print(f\"{num_correct} / {num_examples} correct {100.0 * (num_correct / num_examples)}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QB4Xb6KBRaa",
        "outputId": "88620b2c-3900-4499-fe06-839c03080ea6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9586 / 10000 correct 95.86%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disassembling ðŸ˜¥\n",
        "Sometimes it's cool to know what's happening under the hood so, by using the ``disassemble`` function you can disassemble the code produced be the backend into a slightly friendly form. A slightly outdated description of the instruction set is provided at https://github.com/neworderofjamie/riscv_ise/blob/master/docs/instruction_set.pdf"
      ],
      "metadata": {
        "id": "UkvnOrSY-oGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyfenn import disassemble, init_logging\n",
        "for i, c in enumerate(code):\n",
        "    print(f\"{i * 4} : {disassemble(c)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIMwjWBE-auW",
        "outputId": "7cdbf4f9-6104-4a42-b9b6-507fca4b2d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : ADDI X1, X0, 0\n",
            "4 : ADDI X2, X0, 79\n",
            "8 : LW X7, 44(X0)\n",
            "12 : ADDI X8, X0, 16\n",
            "16 : ADD X3, X8, X7\n",
            "20 : LW X8, 52(X0)\n",
            "24 : ADDI X9, X0, 64\n",
            "28 : ADDI X5, X0, 1\n",
            "32 : ADDI X4, X0, 31\n",
            "36 : LW X6, 0(X7)\n",
            "40 : ADDI X7, X7, 4\n",
            "44 : BEQ X6, X0, 80\n",
            "48 : ADDI X10, X4, 0\n",
            "52 : CLZ X11, X6, 1536\n",
            "56 : BEQ X6, X5, 80\n",
            "60 : ADDI X12, X11, 1\n",
            "64 : SLL X6, X6, X12\n",
            "68 : SUB X10, X10, X11\n",
            "72 : LW X8, 52(X0)\n",
            "76 : LW X13, 48(X0)\n",
            "80 : MUL X14, X10, X9\n",
            "84 : ADD X13, X13, X14\n",
            "88 : VLOAD V1, 0(X8)\n",
            "92 : ADDI X14, X0, 1023\n",
            "96 : VLOAD V0, 0(X13)\n",
            "100 : VLOAD V2, 64(X8)\n",
            "104 : VADD_S V3, V1, V0\n",
            "108 : VSEL V1, X14, V3\n",
            "112 : VSTORE V1, 0(X8)\n",
            "116 : ADDI X10, X10, -1\n",
            "120 : BNE X6, X0, -68\n",
            "124 : ADDI X4, X4, 32\n",
            "128 : BNE X7, X3, -92\n",
            "132 : BEQ X0, X0, 12\n",
            "136 : ADDI X6, X0, 0\n",
            "140 : BEQ X0, X0, -72\n",
            "144 : LW X7, 32(X0)\n",
            "148 : ADDI X8, X0, 100\n",
            "152 : MUL X9, X1, X8\n",
            "156 : ADD X7, X7, X9\n",
            "160 : ADD X3, X8, X7\n",
            "164 : LW X8, 40(X0)\n",
            "168 : ADDI X9, X0, 256\n",
            "172 : ADDI X5, X0, 1\n",
            "176 : ADDI X4, X0, 31\n",
            "180 : LW X6, 0(X7)\n",
            "184 : ADDI X7, X7, 4\n",
            "188 : BEQ X6, X0, 124\n",
            "192 : ADDI X10, X4, 0\n",
            "196 : CLZ X11, X6, 1536\n",
            "200 : BEQ X6, X5, 124\n",
            "204 : ADDI X12, X11, 1\n",
            "208 : SLL X6, X6, X12\n",
            "212 : SUB X10, X10, X11\n",
            "216 : LW X8, 40(X0)\n",
            "220 : LW X13, 36(X0)\n",
            "224 : MUL X14, X10, X9\n",
            "228 : ADD X13, X13, X14\n",
            "232 : VLOAD V1, 0(X8)\n",
            "236 : ADDI X14, X8, 256\n",
            "240 : VLOAD V0, 0(X13)\n",
            "244 : VLOAD V2, 64(X8)\n",
            "248 : VADD_S V1, V1, V0\n",
            "252 : VSTORE V1, 0(X8)\n",
            "256 : VLOAD V0, 64(X13)\n",
            "260 : VLOAD V1, 128(X8)\n",
            "264 : VADD_S V2, V2, V0\n",
            "268 : VSTORE V2, 64(X8)\n",
            "272 : VLOAD V0, 128(X13)\n",
            "276 : VLOAD V2, 192(X8)\n",
            "280 : VADD_S V1, V1, V0\n",
            "284 : VSTORE V1, 128(X8)\n",
            "288 : VLOAD V0, 192(X13)\n",
            "292 : VLOAD V1, 256(X8)\n",
            "296 : VADD_S V2, V2, V0\n",
            "300 : VSTORE V2, 192(X8)\n",
            "304 : ADDI X10, X10, -1\n",
            "308 : BNE X6, X0, -112\n",
            "312 : ADDI X4, X4, 32\n",
            "316 : BNE X7, X3, -136\n",
            "320 : BEQ X0, X0, 12\n",
            "324 : ADDI X6, X0, 0\n",
            "328 : BEQ X0, X0, -116\n",
            "332 : VLUI V1, 1\n",
            "336 : VLUI V0, 0\n",
            "340 : LW X3, 0(X0)\n",
            "344 : LW X4, 4(X0)\n",
            "348 : LW X5, 8(X0)\n",
            "352 : LW X7, 12(X0)\n",
            "356 : VLUI V2, 5\n",
            "360 : VLUI V3, 20\n",
            "364 : VLUI V4, 30\n",
            "368 : ADDI X6, X5, 256\n",
            "372 : VLOAD V5, 0(X3)\n",
            "376 : VLOAD V6, 0(X4)\n",
            "380 : VLOAD V7, 0(X5)\n",
            "384 : VMUL V8, V4, V7, 5\n",
            "388 : VADD_S V9, V8, V5\n",
            "392 : VADD V7, V9, V0\n",
            "396 : VADD V5, V0, V0\n",
            "400 : VTLT X8, V0, V6\n",
            "404 : VSUB V8, V6, V1\n",
            "408 : VSEL V6, X8, V8\n",
            "412 : XORI X9, X8, -1\n",
            "416 : VTGE X10, V7, V3\n",
            "420 : AND X11, X9, X10\n",
            "424 : SW X11, 0(X7)\n",
            "428 : VSUB_S V8, V7, V3\n",
            "432 : VSEL V7, X11, V8\n",
            "436 : VSEL V6, X11, V2\n",
            "440 : VSTORE V5, 0(X3)\n",
            "444 : VSTORE V6, 0(X4)\n",
            "448 : VSTORE V7, 0(X5)\n",
            "452 : VLOAD V5, 64(X3)\n",
            "456 : VLOAD V6, 64(X4)\n",
            "460 : VLOAD V7, 64(X5)\n",
            "464 : VMUL V8, V4, V7, 5\n",
            "468 : VADD_S V9, V8, V5\n",
            "472 : VADD V7, V9, V0\n",
            "476 : VADD V5, V0, V0\n",
            "480 : VTLT X8, V0, V6\n",
            "484 : VSUB V8, V6, V1\n",
            "488 : VSEL V6, X8, V8\n",
            "492 : XORI X9, X8, -1\n",
            "496 : VTGE X10, V7, V3\n",
            "500 : AND X11, X9, X10\n",
            "504 : SW X11, 4(X7)\n",
            "508 : VSUB_S V8, V7, V3\n",
            "512 : VSEL V7, X11, V8\n",
            "516 : VSEL V6, X11, V2\n",
            "520 : VSTORE V5, 64(X3)\n",
            "524 : VSTORE V6, 64(X4)\n",
            "528 : VSTORE V7, 64(X5)\n",
            "532 : VLOAD V5, 128(X3)\n",
            "536 : VLOAD V6, 128(X4)\n",
            "540 : VLOAD V7, 128(X5)\n",
            "544 : VMUL V8, V4, V7, 5\n",
            "548 : VADD_S V9, V8, V5\n",
            "552 : VADD V7, V9, V0\n",
            "556 : VADD V5, V0, V0\n",
            "560 : VTLT X8, V0, V6\n",
            "564 : VSUB V8, V6, V1\n",
            "568 : VSEL V6, X8, V8\n",
            "572 : XORI X9, X8, -1\n",
            "576 : VTGE X10, V7, V3\n",
            "580 : AND X11, X9, X10\n",
            "584 : SW X11, 8(X7)\n",
            "588 : VSUB_S V8, V7, V3\n",
            "592 : VSEL V7, X11, V8\n",
            "596 : VSEL V6, X11, V2\n",
            "600 : VSTORE V5, 128(X3)\n",
            "604 : VSTORE V6, 128(X4)\n",
            "608 : VSTORE V7, 128(X5)\n",
            "612 : VLOAD V5, 192(X3)\n",
            "616 : VLOAD V6, 192(X4)\n",
            "620 : VLOAD V7, 192(X5)\n",
            "624 : VMUL V8, V4, V7, 5\n",
            "628 : VADD_S V9, V8, V5\n",
            "632 : VADD V7, V9, V0\n",
            "636 : VADD V5, V0, V0\n",
            "640 : VTLT X8, V0, V6\n",
            "644 : VSUB V8, V6, V1\n",
            "648 : VSEL V6, X8, V8\n",
            "652 : XORI X9, X8, -1\n",
            "656 : VTGE X10, V7, V3\n",
            "660 : AND X11, X9, X10\n",
            "664 : SW X11, 12(X7)\n",
            "668 : VSUB_S V8, V7, V3\n",
            "672 : VSEL V7, X11, V8\n",
            "676 : VSEL V6, X11, V2\n",
            "680 : VSTORE V5, 192(X3)\n",
            "684 : VSTORE V6, 192(X4)\n",
            "688 : VSTORE V7, 192(X5)\n",
            "692 : VLUI V0, 0\n",
            "696 : LW X3, 16(X0)\n",
            "700 : LW X4, 20(X0)\n",
            "704 : LW X5, 24(X0)\n",
            "708 : LW X6, 28(X0)\n",
            "712 : VLUI V1, 2\n",
            "716 : VLUI V2, 61\n",
            "720 : ADDI X7, X0, 1023\n",
            "724 : VLOAD V3, 0(X3)\n",
            "728 : VLOAD V4, 0(X4)\n",
            "732 : VLOAD V5, 0(X5)\n",
            "736 : VLOAD V6, 0(X6)\n",
            "740 : VMUL V7, V2, V6, 6\n",
            "744 : VADD_S V8, V7, V4\n",
            "748 : VADD_S V7, V8, V3\n",
            "752 : VADD V6, V7, V0\n",
            "756 : VADD V4, V0, V0\n",
            "760 : VMUL V7, V1, V6, 6\n",
            "764 : VADD_S V5, V5, V7\n",
            "768 : VSTORE V3, 0(X3)\n",
            "772 : VSTORE V4, 0(X4)\n",
            "776 : VSTORE V5, 0(X5)\n",
            "780 : VSTORE V6, 0(X6)\n",
            "784 : ADDI X1, X1, 1\n",
            "788 : BNE X1, X2, -780\n",
            "792 : None\n"
          ]
        }
      ]
    }
  ]
}