{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOySFqqnWcVrsr9SuJ4wGMW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ef76342e0e144786bac47c2fef8d2294": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e57be20995724562af7e32dac14a69f6",
              "IPY_MODEL_536f388d11754bfa897d0a56a5a893e3",
              "IPY_MODEL_afa6a6cadd394db68ee93dd757bc7e4e"
            ],
            "layout": "IPY_MODEL_48828ecbf07841118d191cc57aed1305"
          }
        },
        "e57be20995724562af7e32dac14a69f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dd2b43bbcd440dfb16ac5078af76af3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_18fe0a6b9954477880aa34b6b97c76f3",
            "value": "100%"
          }
        },
        "536f388d11754bfa897d0a56a5a893e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4694efe8a12f4827a8d2acdcbf1a7ecd",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_adcd0d60424f4b63822328381168f6b2",
            "value": 10000
          }
        },
        "afa6a6cadd394db68ee93dd757bc7e4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b920a9aa47fc4917835200254d3ccbc0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5f199d0ef6124bfebd78b38881cb307b",
            "value": "â€‡10000/10000â€‡[00:11&lt;00:00,â€‡846.77it/s]"
          }
        },
        "48828ecbf07841118d191cc57aed1305": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dd2b43bbcd440dfb16ac5078af76af3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18fe0a6b9954477880aa34b6b97c76f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4694efe8a12f4827a8d2acdcbf1a7ecd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adcd0d60424f4b63822328381168f6b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b920a9aa47fc4917835200254d3ccbc0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f199d0ef6124bfebd78b38881cb307b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/neworderofjamie/riscv_ise/blob/master/tutorials/mnist_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are lots of rough edges here: error checking is lacking in places, the compiler supports an even smaller subset of C than it should and the wrapping of various bits of API is not very Pythonic.\n",
        "\n",
        "# Installation\n",
        "The current prototype FeNN toolchain is a little bit tricky to build as it re-uses parts of GeNN (mostly the type system and the GeNNCode scanner, parser and type checker) so, on colab, we can install a prebuilt wheel from my google drive:"
      ],
      "metadata": {
        "id": "zpi4-Zy__TDB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zE_bp-iU2ZaT",
        "outputId": "bb95ddb1-3988-43bf-903e-75d7ee9c2033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1aO3CLhWJeoDXJ-lb7FqrtxDy-7WRsNYK\n",
            "To: /content/pyfenn-0.0.1-cp312-cp312-linux_x86_64.whl\n",
            "\r  0% 0.00/6.55M [00:00<?, ?B/s]\r100% 6.55M/6.55M [00:00<00:00, 112MB/s]\n",
            "Processing ./pyfenn-0.0.1-cp312-cp312-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from pyfenn==0.0.1) (2.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from pyfenn==0.0.1) (5.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from pyfenn==0.0.1) (75.2.0)\n",
            "Installing collected packages: pyfenn\n",
            "Successfully installed pyfenn-0.0.1\n"
          ]
        }
      ],
      "source": [
        "if \"google.colab\" in str(get_ipython()):\n",
        "    !gdown 1aO3CLhWJeoDXJ-lb7FqrtxDy-7WRsNYK\n",
        "    !pip install pyfenn-0.0.1-cp312-cp312-linux_x86_64.whl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/neworderofjamie/riscv_ise/raw/refs/heads/master/bin/mnist_bias.bin\n",
        "!wget -q https://github.com/neworderofjamie/riscv_ise/raw/refs/heads/master/bin/mnist_in_hid.bin\n",
        "!wget -q https://github.com/neworderofjamie/riscv_ise/raw/refs/heads/master/bin/mnist_hid_out.bin"
      ],
      "metadata": {
        "id": "q2Nco5kXEG1w"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the trusty mnist package so we can easily access a dataset:"
      ],
      "metadata": {
        "id": "qCuUjyrm_pZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mnist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYEvCEXj7LWT",
        "outputId": "0844e841-2dea-449c-9961-2fcbcc8d20a8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mnist\n",
            "  Downloading mnist-0.2.2-py2.py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mnist) (2.0.2)\n",
            "Downloading mnist-0.2.2-py2.py3-none-any.whl (3.5 kB)\n",
            "Installing collected packages: mnist\n",
            "Successfully installed mnist-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n",
        "Import a bunch of stuff from PyFeNN:"
      ],
      "metadata": {
        "id": "4cgxlc-R_vCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mnist\n",
        "import numpy\n",
        "import numpy as np\n",
        "\n",
        "from pyfenn import (BackendFeNNSim, EventContainer, Model, NeuronUpdateProcess,\n",
        "                    Parameter, ProcessGroup, Runtime, Variable)\n",
        "from pyfenn.models import Linear, Memset\n",
        "\n",
        "from pyfenn import init_logging\n",
        "from pyfenn.utils import get_array_view, get_latency_spikes, load_and_push, zero_and_push\n",
        "from tqdm.auto import trange"
      ],
      "metadata": {
        "id": "TUMEw7-x4buT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer classes\n",
        "FeNN is programmed using a small number of primitive objects:\n",
        "*   ``Processes`` perform computation\n",
        "*   ``Variables`` are used to hold model state e.g. neuron variables and weights\n",
        "*   ``EventContainers`` are the primary means of communication between neuron processes\n",
        "\n",
        "The FeNN tools don't really enforce any particular style of modelling but you can easily use these primitives to create PyTorchesque layer objects. We start by creating a leaky integrator for the output layer. This integrates an input current + bias into a membrane voltage which is averaged over the trial. The update to be performed each timestep is implemented in a ``NeuronUpdateProcess`` which performs the same update to each neuron (as dictated by the same of the variables). In future, these processes might be Just-in-Time compiled from Python but, right now, they are implemented in [GeNNCode](https://genn-team.github.io/genn/documentation/5/custom_models.html#genncode). This is basically a subset of C with extensions for fixed-point types inspired by the [ISO standard extension](https://www.open-std.org/jtc1/sc22/wg14/www/docs/n1005.pdf). In the ``LI`` model, this is most obvious is the ``0.0h6`` literal suffix which indicates that this is a fixed point literal with 6 fractional bits (type promotion doesn't work 100% right now...):"
      ],
      "metadata": {
        "id": "DR1TCosBFf-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LI:\n",
        "    def __init__(self, shape, tau_m: float, num_timesteps: int):\n",
        "        self.shape = shape\n",
        "        dtype = \"s9_6_sat_t\"\n",
        "\n",
        "        self.v = Variable(self.shape, dtype)\n",
        "        self.i = Variable(self.shape, dtype)\n",
        "        self.v_avg = Variable(self.shape, dtype)\n",
        "        self.bias = Variable(self.shape, dtype)\n",
        "        self.process = NeuronUpdateProcess(\n",
        "            \"\"\"\n",
        "            V = (Alpha * V) + I + Bias;\n",
        "            I = 0.0h6;\n",
        "            VAvg += (VAvgScale * V);\n",
        "            \"\"\",\n",
        "            {\"Alpha\": Parameter(np.exp(-1.0 / tau_m), dtype),\n",
        "             \"VAvgScale\": Parameter(1.0 / (num_timesteps / 2), dtype)},\n",
        "            {\"V\": self.v, \"VAvg\": self.v_avg, \"I\": self.i, \"Bias\": self.bias})\n"
      ],
      "metadata": {
        "id": "wGOEN7Ow6nCL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Leaky Integrate-and-Fire model we use for the hidden layer is slightly more complex, but is defined in basically the same way. Because the LIF neuron emits spikes, as well as variables, it has an ``EventContainer`` to manage the emitted spike. In the process code, events are emitted by calling the name of assigned to the event container i.e. ``Spike()``:"
      ],
      "metadata": {
        "id": "u_brKjkxIqdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LIF:\n",
        "    def __init__(self, shape, tau_m: float, tau_refrac: int, v_thresh: float):\n",
        "        self.shape = shape\n",
        "        dtype =  \"s10_5_sat_t\"\n",
        "        self.v = Variable(self.shape, dtype)\n",
        "        self.i = Variable(self.shape, dtype)\n",
        "        self.refrac_time = Variable(self.shape, \"int16_t\")\n",
        "        self.out_spikes = EventContainer(self.shape)\n",
        "        self.process = NeuronUpdateProcess(\n",
        "            \"\"\"\n",
        "            V = (Alpha * V) + I;\n",
        "            I = 0.0h5;\n",
        "            if (RefracTime > 0) {\n",
        "               RefracTime -= 1;\n",
        "            }\n",
        "            else if(V >= VThresh) {\n",
        "               Spike();\n",
        "               V -= VThresh;\n",
        "               RefracTime = TauRefrac;\n",
        "            }\n",
        "            \"\"\",\n",
        "            {\"Alpha\": Parameter(np.exp(-1.0 / tau_m), dtype),\n",
        "             \"VThresh\": Parameter(v_thresh, dtype),\n",
        "             \"TauRefrac\": Parameter(tau_refrac, \"int16_t\")},\n",
        "            {\"V\": self.v, \"I\": self.i, \"RefracTime\": self.refrac_time},\n",
        "            {\"Spike\": self.out_spikes})"
      ],
      "metadata": {
        "id": "_KsNi8Ov6ddG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ],
      "metadata": {
        "id": "ndJkztL9FkQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_examples = 10000\n",
        "num_timesteps = 79\n",
        "input_shape = 28 * 28\n",
        "hidden_shape = 128\n",
        "output_shape = 10\n",
        "input_hidden_shape = [input_shape, hidden_shape]\n",
        "hidden_output_shape = [hidden_shape, output_shape]"
      ],
      "metadata": {
        "id": "AR1n7maS6tb2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "Convert MNIST into a latency. Yan LeCun's original site has been down for some time/blocking colab so we override"
      ],
      "metadata": {
        "id": "E49PKLzA64O4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist.datasets_url = \"https://storage.googleapis.com/cvdf-datasets/mnist/\"\n",
        "mnist_spikes = get_latency_spikes(mnist.test_images())\n",
        "mnist_labels = mnist.test_labels().astype(np.int16)\n"
      ],
      "metadata": {
        "id": "t1qrVC-x6taL",
        "outputId": "a84d3880-33ac-43a4-ca7e-bb4abcedbc93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pyfenn/utils.py:148: RuntimeWarning: divide by zero encountered in log\n",
            "  times = np.round(tau * np.log(i / (i - threshold))).astype(int)\n",
            "/usr/local/lib/python3.12/dist-packages/pyfenn/utils.py:148: RuntimeWarning: invalid value encountered in cast\n",
            "  times = np.round(tau * np.log(i / (i - threshold))).astype(int)\n",
            "/usr/local/lib/python3.12/dist-packages/pyfenn/utils.py:148: RuntimeWarning: divide by zero encountered in divide\n",
            "  times = np.round(tau * np.log(i / (i - threshold))).astype(int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model definition\n",
        "The FeNN tools can produce lots of helpful logging information so we initialise this system before we do anything else (if you use ``from pyfenn import PlogSeverity`` to import the enum you can then use e.g. ``PlogSeverity.DEBUG`` to control the logging level):"
      ],
      "metadata": {
        "id": "6Q4AILkYCEmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "init_logging()"
      ],
      "metadata": {
        "id": "-z5TKUCLCByo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input spikes can be directly injected into FeNN rather than needing any sort of layer so define an EventContainer to hold them"
      ],
      "metadata": {
        "id": "b4tz2VYN7Wlc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_spikes = EventContainer(input_shape, num_timesteps)"
      ],
      "metadata": {
        "id": "wEE6sPpe7V5a"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then create hidden and output layers using the classes we defined above. The fixed-point types are specified as strings, for example s10_5_sat_t is a signed 16-bit fixed point type (this is all FeNN currently supports) with 10 integer and 5 fractional bits to which saturation should be applied (currently only when adding and subtracting):"
      ],
      "metadata": {
        "id": "WgjyF0qj7mOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden = LIF(hidden_shape, 20.0, 5, 0.61)\n",
        "output = LI(output_shape, 20.0, num_timesteps)"
      ],
      "metadata": {
        "id": "NLIaZc0O6x6v"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we connect spiking outputs to input variables using the linear layer class we defined earlier:"
      ],
      "metadata": {
        "id": "IQXNwWGB8zi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_hidden = Linear(input_spikes, hidden.i, \"s10_5_sat_t\")\n",
        "hidden_output = Linear(hidden.out_spikes, output.i, \"s9_6_sat_t\")"
      ],
      "metadata": {
        "id": "oQBHvf-W8ets"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, to reduce the amount of data movement between the FPGA and the CPU, we define a process to zero the classifcation output at the beginning of every trial"
      ],
      "metadata": {
        "id": "VNb3n0MjN9jE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "avg_zero = Memset(output.v_avg)"
      ],
      "metadata": {
        "id": "baFEIjXjN8XS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Process groups define computation that can be performed in parallel (in fact, on FeNN it's not but this won't be the case with e.g. GPU backends) so we group our neuron update processes, event propagation and zeroing processes into seperate groups"
      ],
      "metadata": {
        "id": "GsqvoBeZ9O86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "neuron_update_processes = ProcessGroup([hidden.process, output.process])\n",
        "synapse_update_processes = ProcessGroup([input_hidden.process, hidden_output.process])\n",
        "zero_processes = ProcessGroup([avg_zero.process])\n"
      ],
      "metadata": {
        "id": "5GJbeSgS9Hr-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define a model which groups together all parts of our simulation:"
      ],
      "metadata": {
        "id": "hXDuBp2B9kKK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulation\n",
        "Sadly Google has yet to install FeNN nodes into it's cloud so for now we create a simulation backend (if you are lucky enough to be running on a Kria KV260 with the bitstream loaded, you should substitute ``BackendFeNNHW`` here) and use it to create a generic simulation kernel. The control flow of these kernels *will* be fully programmable but for now you can either create a really simple kernel which just runs a list of process groups or a 'simulation' kernel which offloads running a loop over time with a list of process groups in the body and seperate lists that runs at the beginning (which we use here to zero the output average) and end."
      ],
      "metadata": {
        "id": "trpGDMqC9zwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backend = BackendFeNNSim()\n",
        "model = Model([neuron_update_processes, synapse_update_processes, zero_processes], backend)\n",
        "code = backend.generate_simulation_kernel([synapse_update_processes, neuron_update_processes],\n",
        "                                          [zero_processes], [],\n",
        "                                          num_timesteps, model)"
      ],
      "metadata": {
        "id": "5qTO9zPZ9v1Y"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have some code, we create a ``Runtime`` object to interact with the FeNN. We first use this to allocate the memory required for our model on FeNN:"
      ],
      "metadata": {
        "id": "y94UlN66_7XS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "runtime = Runtime(model, backend)\n",
        "runtime.allocate()"
      ],
      "metadata": {
        "id": "oAU6n6xN_Kfw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we use some helper functions to load weights into the appropriate variables:"
      ],
      "metadata": {
        "id": "IenSfYE5AneX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "load_and_push(\"mnist_in_hid.bin\", input_hidden.weight, runtime)\n",
        "load_and_push(\"mnist_hid_out.bin\", hidden_output.weight, runtime)\n",
        "load_and_push(\"mnist_bias.bin\", output.bias, runtime)"
      ],
      "metadata": {
        "id": "bdYJRLN8Agsq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and set the remaining variables to zero:"
      ],
      "metadata": {
        "id": "YcJqRtw2ArbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zero_and_push(hidden.v, runtime)\n",
        "zero_and_push(hidden.i, runtime)\n",
        "zero_and_push(hidden.refrac_time, runtime)\n",
        "zero_and_push(output.v, runtime)\n",
        "zero_and_push(output.i, runtime)"
      ],
      "metadata": {
        "id": "ZYhU6-15AwdU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we upload the code generated by the backend to FeNN:"
      ],
      "metadata": {
        "id": "Cp8gi-giA0Ko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "runtime.set_instructions(code)"
      ],
      "metadata": {
        "id": "t6qjbAi9AzyZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ``Runtime`` object creates a bunch of 'Array' objects which are used to interact with model state at runtime. To save typing later on, we look these up now:"
      ],
      "metadata": {
        "id": "_DFSZgyEA-kz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_spike_array, input_spike_view = get_array_view(runtime, input_spikes,\n",
        "                                                     np.uint32)\n",
        "hidden_spike_array = runtime.get_array(hidden.out_spikes)\n",
        "\n",
        "output_v_avg_array, output_v_avg_view  = get_array_view(runtime, output.v_avg, np.int16)"
      ],
      "metadata": {
        "id": "yiyDQpO2A9KC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we're ready to go! Now we can loop through the MNIST digits and:\n",
        "1.   Copy each digit into the input spike array\n",
        "2.   Run the kernel\n",
        "3.   Copy the averaged output voltage back from FeNN\n",
        "4.   Check whether this matches the correct label"
      ],
      "metadata": {
        "id": "-81zZEvlBWa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_correct = 0\n",
        "for i in trange(num_examples):\n",
        "    # Copy data to array host pointe\n",
        "    input_spike_view[:] = mnist_spikes[i]\n",
        "    input_spike_array.push_to_device();\n",
        "\n",
        "    # Classify\n",
        "    runtime.run()\n",
        "\n",
        "    # Copy output V sum from device\n",
        "    output_v_avg_array.pull_from_device();\n",
        "\n",
        "    # Determine if output is correct\n",
        "    classification = np.argmax(output_v_avg_view)\n",
        "    if classification == mnist_labels[i]:\n",
        "        num_correct += 1\n",
        "\n",
        "print(f\"{num_correct} / {num_examples} correct {100.0 * (num_correct / num_examples)}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "ef76342e0e144786bac47c2fef8d2294",
            "e57be20995724562af7e32dac14a69f6",
            "536f388d11754bfa897d0a56a5a893e3",
            "afa6a6cadd394db68ee93dd757bc7e4e",
            "48828ecbf07841118d191cc57aed1305",
            "8dd2b43bbcd440dfb16ac5078af76af3",
            "18fe0a6b9954477880aa34b6b97c76f3",
            "4694efe8a12f4827a8d2acdcbf1a7ecd",
            "adcd0d60424f4b63822328381168f6b2",
            "b920a9aa47fc4917835200254d3ccbc0",
            "5f199d0ef6124bfebd78b38881cb307b"
          ]
        },
        "id": "6QB4Xb6KBRaa",
        "outputId": "4d6f18ba-428b-49b4-a9de-914df48d8213"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef76342e0e144786bac47c2fef8d2294"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9583 / 10000 correct 95.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Disassembling ðŸ˜¥\n",
        "Sometimes it's cool to know what's happening under the hood so, by using the ``disassemble`` function you can disassemble the code produced be the backend into a slightly friendly form. A slightly outdated description of the instruction set is provided at https://github.com/neworderofjamie/riscv_ise/blob/master/docs/instruction_set.pdf"
      ],
      "metadata": {
        "id": "UkvnOrSY-oGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyfenn import disassemble, init_logging\n",
        "for i, c in enumerate(code):\n",
        "    print(f\"{i * 4} : {disassemble(c)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIMwjWBE-auW",
        "outputId": "3253f44f-240d-413f-e5bf-c2977ab3380f"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 : ADDI X1, X0, 0\n",
            "4 : ADDI X2, X0, 79\n",
            "8 : LW X3, 60(X0)\n",
            "12 : VLUI V0, 0\n",
            "16 : VSTORE V0, 0(X3)\n",
            "20 : LW X3, 48(X0)\n",
            "24 : ADDI X5, X0, 16\n",
            "28 : ADD X4, X5, X3\n",
            "32 : ADDI X5, X0, 64\n",
            "36 : ADDI X7, X0, 1\n",
            "40 : ADDI X6, X0, 31\n",
            "44 : LW X8, 0(X3)\n",
            "48 : ADDI X3, X3, 4\n",
            "52 : BEQ X8, X0, 80\n",
            "56 : ADDI X9, X6, 0\n",
            "60 : CLZ X10, X8\n",
            "64 : BEQ X8, X7, 80\n",
            "68 : ADDI X11, X10, 1\n",
            "72 : SLL X8, X8, X11\n",
            "76 : SUB X9, X9, X10\n",
            "80 : LW X12, 52(X0)\n",
            "84 : MUL X13, X9, X5\n",
            "88 : ADD X12, X12, X13\n",
            "92 : LW X13, 56(X0)\n",
            "96 : VLOAD V1, 0(X13)\n",
            "100 : ADDI X14, X0, 1023\n",
            "104 : VLOAD V0, 0(X12)\n",
            "108 : VLOAD V2, 64(X13)\n",
            "112 : VADD_S V3, V1, V0\n",
            "116 : VSEL V1, X14, V3\n",
            "120 : VSTORE V1, 0(X13)\n",
            "124 : ADDI X9, X9, -1\n",
            "128 : BNE X8, X0, -68\n",
            "132 : ADDI X6, X6, 32\n",
            "136 : BNE X3, X4, -92\n",
            "140 : BEQ X0, X0, 12\n",
            "144 : ADDI X8, X0, 0\n",
            "148 : BEQ X0, X0, -72\n",
            "152 : LW X3, 36(X0)\n",
            "156 : ADDI X5, X0, 100\n",
            "160 : MUL X6, X1, X5\n",
            "164 : ADD X3, X3, X6\n",
            "168 : ADD X4, X5, X3\n",
            "172 : ADDI X5, X0, 256\n",
            "176 : ADDI X7, X0, 1\n",
            "180 : ADDI X6, X0, 31\n",
            "184 : LW X8, 0(X3)\n",
            "188 : ADDI X3, X3, 4\n",
            "192 : BEQ X8, X0, 124\n",
            "196 : ADDI X9, X6, 0\n",
            "200 : CLZ X10, X8\n",
            "204 : BEQ X8, X7, 124\n",
            "208 : ADDI X11, X10, 1\n",
            "212 : SLL X8, X8, X11\n",
            "216 : SUB X9, X9, X10\n",
            "220 : LW X12, 40(X0)\n",
            "224 : MUL X13, X9, X5\n",
            "228 : ADD X12, X12, X13\n",
            "232 : LW X13, 44(X0)\n",
            "236 : VLOAD V1, 0(X13)\n",
            "240 : ADDI X14, X13, 256\n",
            "244 : VLOAD V0, 0(X12)\n",
            "248 : VLOAD V2, 64(X13)\n",
            "252 : VADD_S V1, V1, V0\n",
            "256 : VSTORE V1, 0(X13)\n",
            "260 : VLOAD V0, 64(X12)\n",
            "264 : VLOAD V1, 128(X13)\n",
            "268 : VADD_S V2, V2, V0\n",
            "272 : VSTORE V2, 64(X13)\n",
            "276 : VLOAD V0, 128(X12)\n",
            "280 : VLOAD V2, 192(X13)\n",
            "284 : VADD_S V1, V1, V0\n",
            "288 : VSTORE V1, 128(X13)\n",
            "292 : VLOAD V0, 192(X12)\n",
            "296 : VLOAD V1, 256(X13)\n",
            "300 : VADD_S V2, V2, V0\n",
            "304 : VSTORE V2, 192(X13)\n",
            "308 : ADDI X9, X9, -1\n",
            "312 : BNE X8, X0, -112\n",
            "316 : ADDI X6, X6, 32\n",
            "320 : BNE X3, X4, -136\n",
            "324 : BEQ X0, X0, 12\n",
            "328 : ADDI X8, X0, 0\n",
            "332 : BEQ X0, X0, -116\n",
            "336 : VLUI V1, 1\n",
            "340 : VLUI V0, 0\n",
            "344 : LW X4, 4(X0)\n",
            "348 : LW X5, 8(X0)\n",
            "352 : LW X6, 12(X0)\n",
            "356 : LW X7, 16(X0)\n",
            "360 : VLUI V2, 5\n",
            "364 : VLUI V3, 20\n",
            "368 : VLUI V4, 30\n",
            "372 : ADDI X3, X6, 256\n",
            "376 : VLOAD V5, 0(X4)\n",
            "380 : VLOAD V6, 0(X5)\n",
            "384 : VLOAD V7, 0(X6)\n",
            "388 : ADDI X0, X0, 0\n",
            "392 : VMUL_RN V8, V4, V7, 5\n",
            "396 : VADD_S V8, V8, V5\n",
            "400 : VADD V7, V8, V0\n",
            "404 : VADD V5, V0, V0\n",
            "408 : VTLT X8, V0, V6\n",
            "412 : VSUB V8, V6, V1\n",
            "416 : VSEL V6, X8, V8\n",
            "420 : XORI X9, X8, -1\n",
            "424 : VTGE X10, V7, V3\n",
            "428 : AND X11, X9, X10\n",
            "432 : SW X11, 0(X7)\n",
            "436 : VSUB_S V8, V7, V3\n",
            "440 : VSEL V7, X11, V8\n",
            "444 : VSEL V6, X11, V2\n",
            "448 : VSTORE V5, 0(X4)\n",
            "452 : VSTORE V6, 0(X5)\n",
            "456 : VSTORE V7, 0(X6)\n",
            "460 : VLOAD V5, 64(X4)\n",
            "464 : VLOAD V6, 64(X5)\n",
            "468 : VLOAD V7, 64(X6)\n",
            "472 : ADDI X0, X0, 0\n",
            "476 : VMUL_RN V8, V4, V7, 5\n",
            "480 : VADD_S V8, V8, V5\n",
            "484 : VADD V7, V8, V0\n",
            "488 : VADD V5, V0, V0\n",
            "492 : VTLT X8, V0, V6\n",
            "496 : VSUB V8, V6, V1\n",
            "500 : VSEL V6, X8, V8\n",
            "504 : XORI X9, X8, -1\n",
            "508 : VTGE X10, V7, V3\n",
            "512 : AND X11, X9, X10\n",
            "516 : SW X11, 4(X7)\n",
            "520 : VSUB_S V8, V7, V3\n",
            "524 : VSEL V7, X11, V8\n",
            "528 : VSEL V6, X11, V2\n",
            "532 : VSTORE V5, 64(X4)\n",
            "536 : VSTORE V6, 64(X5)\n",
            "540 : VSTORE V7, 64(X6)\n",
            "544 : VLOAD V5, 128(X4)\n",
            "548 : VLOAD V6, 128(X5)\n",
            "552 : VLOAD V7, 128(X6)\n",
            "556 : ADDI X0, X0, 0\n",
            "560 : VMUL_RN V8, V4, V7, 5\n",
            "564 : VADD_S V8, V8, V5\n",
            "568 : VADD V7, V8, V0\n",
            "572 : VADD V5, V0, V0\n",
            "576 : VTLT X8, V0, V6\n",
            "580 : VSUB V8, V6, V1\n",
            "584 : VSEL V6, X8, V8\n",
            "588 : XORI X9, X8, -1\n",
            "592 : VTGE X10, V7, V3\n",
            "596 : AND X11, X9, X10\n",
            "600 : SW X11, 8(X7)\n",
            "604 : VSUB_S V8, V7, V3\n",
            "608 : VSEL V7, X11, V8\n",
            "612 : VSEL V6, X11, V2\n",
            "616 : VSTORE V5, 128(X4)\n",
            "620 : VSTORE V6, 128(X5)\n",
            "624 : VSTORE V7, 128(X6)\n",
            "628 : VLOAD V5, 192(X4)\n",
            "632 : VLOAD V6, 192(X5)\n",
            "636 : VLOAD V7, 192(X6)\n",
            "640 : ADDI X0, X0, 0\n",
            "644 : VMUL_RN V8, V4, V7, 5\n",
            "648 : VADD_S V8, V8, V5\n",
            "652 : VADD V7, V8, V0\n",
            "656 : VADD V5, V0, V0\n",
            "660 : VTLT X8, V0, V6\n",
            "664 : VSUB V8, V6, V1\n",
            "668 : VSEL V6, X8, V8\n",
            "672 : XORI X9, X8, -1\n",
            "676 : VTGE X10, V7, V3\n",
            "680 : AND X11, X9, X10\n",
            "684 : SW X11, 12(X7)\n",
            "688 : VSUB_S V8, V7, V3\n",
            "692 : VSEL V7, X11, V8\n",
            "696 : VSEL V6, X11, V2\n",
            "700 : VSTORE V5, 192(X4)\n",
            "704 : VSTORE V6, 192(X5)\n",
            "708 : VSTORE V7, 192(X6)\n",
            "712 : VLUI V0, 0\n",
            "716 : LW X7, 20(X0)\n",
            "720 : LW X8, 24(X0)\n",
            "724 : LW X9, 28(X0)\n",
            "728 : LW X10, 32(X0)\n",
            "732 : VLUI V1, 2\n",
            "736 : VLUI V2, 61\n",
            "740 : ADDI X3, X0, 1023\n",
            "744 : VLOAD V3, 0(X7)\n",
            "748 : VLOAD V4, 0(X8)\n",
            "752 : VLOAD V5, 0(X9)\n",
            "756 : VLOAD V6, 0(X10)\n",
            "760 : ADDI X0, X0, 0\n",
            "764 : VMUL_RN V7, V2, V6, 6\n",
            "768 : VADD_S V7, V7, V4\n",
            "772 : VADD_S V7, V7, V3\n",
            "776 : VADD V6, V7, V0\n",
            "780 : VADD V4, V0, V0\n",
            "784 : VMUL_RN V7, V1, V6, 6\n",
            "788 : VADD_S V5, V5, V7\n",
            "792 : VSTORE V3, 0(X7)\n",
            "796 : VSTORE V4, 0(X8)\n",
            "800 : VSTORE V5, 0(X9)\n",
            "804 : VSTORE V6, 0(X10)\n",
            "808 : ADDI X1, X1, 1\n",
            "812 : BNE X1, X2, -792\n",
            "816 : ECALL\n"
          ]
        }
      ]
    }
  ]
}